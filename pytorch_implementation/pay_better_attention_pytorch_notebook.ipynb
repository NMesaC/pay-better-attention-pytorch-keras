{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The below cell has the Super Attention Layer that has a single W_a matrix managed by the Multi Head Attention Module. Run it if you want to use that version of Super Attention (all other attention layers are the same)"
      ],
      "metadata": {
        "id": "PbfyxKZ1__QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model : int,\n",
        "                 d_q : int,\n",
        "                 d_k : int,\n",
        "                 d_v : int,\n",
        "                 W_a : nn.Linear = None,\n",
        "                 layer_type : str = 'SDPA',\n",
        "                 idx : int = 0,\n",
        "                 max_len : int = 32):\n",
        "        super().__init__()\n",
        "        self.d_model    = d_model\n",
        "        self.d_q        = d_q\n",
        "        self.d_k        = d_k\n",
        "        self.d_v        = d_v\n",
        "        self.layer_type = layer_type\n",
        "        self.idx        = idx\n",
        "        self.max_len    = max_len\n",
        "        self.W_a        = W_a\n",
        "        self._set_layer_type()\n",
        "\n",
        "    def _set_layer_type(self):\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        self.W_q     = nn.Linear(self.d_model,self.d_q)\n",
        "        nn.init.xavier_uniform_(self.W_q.weight)\n",
        "        nn.init.constant_(self.W_q.bias, 0)\n",
        "        if self.layer_type == 'Optimised':\n",
        "            self.W_k     = nn.Linear(self.d_model,self.d_k)\n",
        "            nn.init.xavier_uniform_(self.W_k.weight)\n",
        "            nn.init.constant_(self.W_k.bias, 0)\n",
        "            self.forward = self._forward_optimised\n",
        "        elif self.layer_type == 'Efficient':\n",
        "            self.forward = self._forward_efficient\n",
        "        elif self.layer_type == 'Super':\n",
        "            self.forward = self._forward_super\n",
        "        else:\n",
        "            # Default to SDPA\n",
        "            self.W_k     = nn.Linear(self.d_model,self.d_k)\n",
        "            self.W_v     = nn.Linear(self.d_model,self.d_v)\n",
        "            nn.init.xavier_uniform_(self.W_k.weight)\n",
        "            nn.init.constant_(self.W_k.bias, 0)\n",
        "            nn.init.xavier_uniform_(self.W_v.weight)\n",
        "            nn.init.constant_(self.W_v.bias, 0)\n",
        "            self.forward = self._forward_SDPA\n",
        "\n",
        "    def _forward_SDPA(self, inp_q, inp_k, inp_v):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        K     = self.W_k(inp_k)\n",
        "        V     = self.W_v(inp_v)\n",
        "        K_t   = K.permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_optimised(self, inp_q : torch.Tensor, inp_k : torch.Tensor, inp_v : torch.Tensor):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        K     = self.W_k(inp_k)\n",
        "        K_t   = K.permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        v_lo  = ((self.idx) * self.d_v)\n",
        "        v_hi  = ((self.idx + 1) * self.d_v)\n",
        "        V     = inp_v[:,:, v_lo : v_hi]\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_efficient(self, inp_q : torch.Tensor, inp_k : torch.Tensor, inp_v : torch.Tensor):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        lo    = ((self.idx) * self.d_k)\n",
        "        hi    = ((self.idx + 1) * self.d_k)\n",
        "        K_t   = inp_k[:, :, lo : hi].permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        V     = inp_v[:,:, lo : hi]\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_super(self, inp_q : torch.Tensor, inp_k : torch.Tensor, inp_v : torch.Tensor):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        lo    = ((self.idx) * self.d_k)\n",
        "        hi    = ((self.idx + 1) * self.d_k)\n",
        "        K_t   = inp_k[:, :, lo : hi].permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        V     = self.W_a(inp_v[:,:, lo : hi].permute(0,2,1)).permute(0,2,1)\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, max_len, layer_type):\n",
        "        super().__init__()\n",
        "        self.layers  = nn.Sequential()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k     = d_k\n",
        "        self.d_v     = d_v\n",
        "        self.W_a     = None\n",
        "        if layer_type == 'Super':\n",
        "            self.W_a = nn.Linear(max_len,max_len)\n",
        "            nn.init.xavier_uniform_(self.W_a.weight)\n",
        "            nn.init.constant_(self.W_a.bias, 0)\n",
        "        for i in range(n_heads):\n",
        "            self.layers.add_module(\"Attention_Layer \"+str(i),\n",
        "                                   AttentionLayer(d_model,d_k,d_k,d_v,self.W_a,layer_type,i,max_len))\n",
        "        self.W_o     = nn.Linear(n_heads * d_v, d_model)\n",
        "\n",
        "    def forward(self, inp_q, inp_k, inp_v):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i == 0:\n",
        "                H = layer(inp_q,inp_k,inp_v)\n",
        "            else:\n",
        "                h_i = layer(inp_q,inp_k,inp_v)\n",
        "                h_cat = (H.clone(),h_i)\n",
        "                H = torch.cat(h_cat,2)\n",
        "        out = self.W_o(H)\n",
        "        return out"
      ],
      "metadata": {
        "id": "e19p294QAA9K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell has the Super Attention Layer that has a single W_a matrix FOR EACH Super Attention Layer, independent of the Multi Head Attention Module. Run it if you want to use that version of Super Attention (all other attention layers are the same)"
      ],
      "metadata": {
        "id": "kGqb6eTcAKX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model : int,\n",
        "                 d_q : int,\n",
        "                 d_k : int,\n",
        "                 d_v : int,\n",
        "                 layer_type : str = 'SDPA',\n",
        "                 idx : int = 0,\n",
        "                 max_len : int = 32):\n",
        "        super().__init__()\n",
        "        self.d_model    = d_model\n",
        "        self.d_q        = d_q\n",
        "        self.d_k        = d_k\n",
        "        self.d_v        = d_v\n",
        "        self.layer_type = layer_type\n",
        "        self.idx        = idx\n",
        "        self.max_len    = max_len\n",
        "        self._set_layer_type()\n",
        "\n",
        "\n",
        "    def _set_layer_type(self):\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        self.W_q     = nn.Linear(self.d_model,self.d_q)\n",
        "        nn.init.xavier_uniform_(self.W_q.weight)\n",
        "        nn.init.constant_(self.W_q.bias, 0)\n",
        "        if self.layer_type == 'Optimised':\n",
        "            self.W_k     = nn.Linear(self.d_model,self.d_k)\n",
        "            nn.init.xavier_uniform_(self.W_k.weight)\n",
        "            nn.init.constant_(self.W_k.bias, 0)\n",
        "            self.forward = self._forward_optimised\n",
        "        elif self.layer_type == 'Efficient':\n",
        "            self.forward = self._forward_efficient\n",
        "        elif self.layer_type == 'Super':\n",
        "            self.forward = self._forward_super\n",
        "            self.W_a     = nn.Linear(self.max_len,self.max_len)\n",
        "            nn.init.xavier_uniform_(self.W_a.weight)\n",
        "            nn.init.constant_(self.W_a.bias, 0)\n",
        "        else:\n",
        "            # Default to SDPA\n",
        "            self.W_k     = nn.Linear(self.d_model,self.d_k)\n",
        "            self.W_v     = nn.Linear(self.d_model,self.d_v)\n",
        "            nn.init.xavier_uniform_(self.W_k.weight)\n",
        "            nn.init.constant_(self.W_k.bias, 0)\n",
        "            nn.init.xavier_uniform_(self.W_v.weight)\n",
        "            nn.init.constant_(self.W_v.bias, 0)\n",
        "            self.forward = self._forward_SDPA\n",
        "\n",
        "    def _forward_SDPA(self, inp_q, inp_k, inp_v):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        K     = self.W_k(inp_k)\n",
        "        V     = self.W_v(inp_v)\n",
        "        K_t   = K.permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_optimised(self, inp_q : torch.Tensor, inp_k : torch.Tensor, inp_v : torch.Tensor):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        K     = self.W_k(inp_k)\n",
        "        K_t   = K.permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        v_lo  = ((self.idx) * self.d_v)\n",
        "        v_hi  = ((self.idx + 1) * self.d_v)\n",
        "        V     = inp_v[:,:, v_lo : v_hi]\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_efficient(self, inp_q : torch.Tensor, inp_k : torch.Tensor, inp_v : torch.Tensor):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        lo    = ((self.idx) * self.d_k)\n",
        "        hi    = ((self.idx + 1) * self.d_k)\n",
        "        K_t   = inp_k[:, :, lo : hi].permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        V     = inp_v[:,:, lo : hi]\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_super(self, inp_q : torch.Tensor, inp_k : torch.Tensor, inp_v : torch.Tensor):\n",
        "        Q     = self.W_q(inp_q)\n",
        "        lo    = ((self.idx) * self.d_k)\n",
        "        hi    = ((self.idx + 1) * self.d_k)\n",
        "        K_t   = inp_k[:, :, lo : hi].permute(0,2,1)\n",
        "        S     = self.softmax((Q @ K_t) / math.sqrt(self.d_q))\n",
        "        V     = self.W_a(inp_v[:,:, lo : hi].permute(0,2,1)).permute(0,2,1)\n",
        "        H     = S @ V\n",
        "        return H\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, max_len, layer_type):\n",
        "        super().__init__()\n",
        "        self.layers  = nn.Sequential()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k     = d_k\n",
        "        self.d_v     = d_v\n",
        "        for i in range(n_heads):\n",
        "            self.layers.add_module(\"Attention_Layer \"+str(i),\n",
        "                                   AttentionLayer(d_model,d_k,d_k,d_v,layer_type,i,max_len))\n",
        "        self.W_o     = nn.Linear(n_heads * d_v, d_model)\n",
        "\n",
        "    def forward(self, inp_q, inp_k, inp_v):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i == 0:\n",
        "                H = layer(inp_q,inp_k,inp_v)\n",
        "            else:\n",
        "                h_i = layer(inp_q,inp_k,inp_v)\n",
        "                h_cat = (H.clone(),h_i)\n",
        "                H = torch.cat(h_cat,2)\n",
        "        out = self.W_o(H)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5Yuu4IRWAP7Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell initializes the Dataloader and runs the main script. Remember to download the IMDB_dataset.csv file from the github repo and either upload it to google colab or to have it in the same working directory locally."
      ],
      "metadata": {
        "id": "yWtFbbghAkpc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJoYVz4170yd",
        "outputId": "31c20fdf-4487-4961-8319-1cb52cfe1501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with layer type: SDPA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch: 0: 100%|██████████| 625/625 [00:06<00:00, 102.35it/s]\n",
            "Test Epoch: 0: 100%|██████████| 63/63 [00:00<00:00, 308.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 0: Train loss: 0.6637 |  Train acc: 0.5527 |                 Val loss: 0.6217 | Val acc: 0.6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch: 1: 100%|██████████| 625/625 [00:05<00:00, 104.47it/s]\n",
            "Test Epoch: 1: 100%|██████████| 63/63 [00:00<00:00, 186.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1: Train loss: 0.573 |  Train acc: 0.6878 |                 Val loss: 0.5672 | Val acc: 0.6905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch: 2: 100%|██████████| 625/625 [00:06<00:00, 102.30it/s]\n",
            "Test Epoch: 2: 100%|██████████| 63/63 [00:00<00:00, 296.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2: Train loss: 0.5118 |  Train acc: 0.7399 |                 Val loss: 0.5506 | Val acc: 0.7015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch: 3:  40%|████      | 250/625 [00:02<00:03, 110.82it/s]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "## \"You Need to Pay Better Attention\" Pytorch Transformer Example\n",
        "\n",
        "## Paper Link: https://arxiv.org/abs/2403.01643\n",
        "\n",
        "## Author: Nicholas Mesa-Cucalon (https://github.com/NMesaC)\n",
        "\"\"\"\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Set device since some classes need info\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(1019)\n",
        "\n",
        "\"\"\"\n",
        "## Dataloader\n",
        "\"\"\"\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, csv_path: str, vocab_size: int = 10000, max_length: int = 200):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.preprocess_data()\n",
        "        self.build_vocabulary()\n",
        "        self.tokenize_reviews()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        self.data['review'] = self.data['review'].apply(self.clean_text)\n",
        "        self.data['sentiment'] = self.data['sentiment'].map({\"positive\": 1, \"negative\": 0})\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"<br\\s*/?>\", \" \", text)\n",
        "        text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "        return text.strip()\n",
        "\n",
        "    def build_vocabulary(self):\n",
        "        word_freq = Counter()\n",
        "        for review in self.data['review']:\n",
        "            word_freq.update(review.split())\n",
        "\n",
        "        special_tokens = ['<PAD>', '<SOS>']\n",
        "        common_words = [word for word, _ in word_freq.most_common(self.vocab_size - len(special_tokens))]\n",
        "        self.vocab = {word: idx for idx, word in enumerate(special_tokens + common_words)}\n",
        "\n",
        "    def tokenize_reviews(self):\n",
        "        self.tokenized_reviews = []\n",
        "        for review in self.data['review']:\n",
        "            tokens = [self.vocab['<SOS>']]\n",
        "            tokens.extend([self.vocab.get(word, self.vocab['<PAD>']) for word in review.split()[:self.max_length-1]])\n",
        "            self.tokenized_reviews.append(torch.tensor(tokens))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        return self.tokenized_reviews[idx], self.data['sentiment'].iloc[idx]\n",
        "\n",
        "def collate_imdb(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    reviews, sentiments = zip(*batch)\n",
        "    padded_reviews = pad_sequence(reviews, batch_first=True, padding_value=0)\n",
        "    lengths = torch.tensor([len(review) for review in reviews])\n",
        "    sentiments = torch.tensor(sentiments, dtype=torch.float32)\n",
        "    return padded_reviews, lengths, sentiments\n",
        "\n",
        "def get_dataloader(csv_path: str, vocab_size: int, max_length: int, batch_size: int, val_split : float) -> Tuple[DataLoader, DataLoader, Dict[str, int]]:\n",
        "    dataset = IMDBDataset(csv_path, vocab_size, max_length)\n",
        "    train_size = 40000\n",
        "    val_size   = (int)(train_size * val_split)\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, len(dataset) - train_size - val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_imdb)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_imdb)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_imdb)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Transformer Block Module\n",
        "\"\"\"\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, layer_type = 'SDPA', max_len = 32, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        d_k, d_v = d_model // num_heads\n",
        "        self.att = MultiHeadAttention(num_heads, d_model, d_k, d_v, max_len, layer_type)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.att(x, x, x)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\"\"\"\n",
        "## Embedding Layer\n",
        "\"\"\"\n",
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        self.pos_emb = nn.Embedding(num_embeddings=maxlen, embedding_dim=embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(self.maxlen, device=x.device)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions.unsqueeze(0)\n",
        "\n",
        "\"\"\"\n",
        "## Transformer-Encoder-Only Arch\n",
        "\"\"\"\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, d_lin, max_len, layer_type, drop_p):\n",
        "        super().__init__()\n",
        "        self.multi      = MultiHeadAttention(n_heads,d_model,d_k,d_v,max_len,layer_type)\n",
        "        self.ff         = nn.Sequential(\n",
        "                            nn.Linear(d_model,d_lin),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear(d_lin,d_model)\n",
        "                         )\n",
        "        self.norm_multi = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm_ff    = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.drop_multi = nn.Dropout(drop_p)\n",
        "        self.drop_ff    = nn.Dropout(drop_p)\n",
        "    def forward(self, inp):\n",
        "        multi = self.multi(inp,inp,inp)\n",
        "        multi = self.drop_multi(multi)\n",
        "        z     = self.norm_multi(inp + multi)\n",
        "        ff    = self.ff(z)\n",
        "        ff    = self.drop_ff(ff)\n",
        "        return self.norm_ff(z + ff)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, d_lin, n_layers, vocab_size, max_len, layer_type, drop_p):\n",
        "        super().__init__()\n",
        "        self.encoder_layers  = nn.Sequential()\n",
        "        self.embedding       = TokenAndPositionEmbedding(max_len,vocab_size,d_model)\n",
        "        self.n_layers        = n_layers\n",
        "        for i in range(n_layers):\n",
        "            self.encoder_layers.add_module(\"Encoder_Layer\"+str(i),EncoderLayer(n_heads,\n",
        "                                                                               d_model,\n",
        "                                                                               d_k,\n",
        "                                                                               d_v,\n",
        "                                                                               d_lin,\n",
        "                                                                               max_len,\n",
        "                                                                               layer_type,\n",
        "                                                                               drop_p))\n",
        "    def forward(self, inp):\n",
        "        embed_i = self.embedding(inp)\n",
        "        for layer in self.encoder_layers:\n",
        "            embed_i     = layer(embed_i)\n",
        "        return embed_i\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size = 20000,\n",
        "                 n_heads    = 4,\n",
        "                 d_model    = 32,\n",
        "                 d_k        = 8,\n",
        "                 d_v        = 8,\n",
        "                 d_lin      = 32,\n",
        "                 n_layers   = 1,\n",
        "                 max_len    = 32,\n",
        "                 layer_type = 'SDPA',\n",
        "                 drop_p     = 0.1):\n",
        "        super().__init__()\n",
        "        d_k = d_model // n_heads\n",
        "        d_v = d_k\n",
        "        self.encoder         = Encoder(n_heads,d_model,d_k,d_v,d_lin,n_layers,vocab_size,max_len,layer_type,drop_p)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout1        = nn.Dropout(0.1)\n",
        "        self.dense1          = nn.Linear(d_model, 6)\n",
        "        self.dropout2        = nn.Dropout(0.1)\n",
        "        self.dense2          = nn.Linear(6, 1)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.encoder(inp)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.global_avg_pool(x).squeeze(2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.dense1(x))\n",
        "        x = self.dropout2(x)\n",
        "        res = self.dense2(x)\n",
        "        return res\n",
        "\n",
        "\"\"\"\n",
        "## Training Loop\n",
        "\"\"\"\n",
        "def train_loop(device, model, optim, loader, loss_func, epoch, train=False):\n",
        "    model.train(mode=train)\n",
        "    total_loss    = 0\n",
        "    total_correct = 0\n",
        "    n_samples     = 0\n",
        "    label         = 'Training' if train else 'Test'\n",
        "    for reviews, _, labels in tqdm(loader, desc=f'{label} Epoch: {epoch}'):\n",
        "        reviews = reviews.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #Forward\n",
        "        if train:\n",
        "            optim.zero_grad()\n",
        "            logits = model(reviews)\n",
        "            loss = loss_func(logits, labels.reshape(logits.shape))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = model(reviews)\n",
        "                loss   = loss_func(logits, labels.reshape(logits.shape))\n",
        "        #Predictions\n",
        "        preds = (logits > 0.5).float()\n",
        "        #Compute accuracy\n",
        "        acc = torch.sum(preds == labels.reshape(logits.shape))\n",
        "        #Track stats\n",
        "        total_loss += reviews.shape[0] * loss\n",
        "        n_samples += reviews.shape[0]\n",
        "        total_correct += acc\n",
        "    return total_loss / n_samples, total_correct / n_samples\n",
        "\n",
        "\"\"\"\n",
        "## Checkpoint Callbacks\n",
        "\"\"\"\n",
        "def save_checkpoint(model, optimizer, epoch, best_metric, filename):\n",
        "    state = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'best_metric': best_metric\n",
        "    }\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    best_metric = checkpoint['best_metric']\n",
        "    return model, optimizer, epoch, best_metric\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        if (\"attention\" in name.lower()) or (\"w_o\" in name.lower()):\n",
        "            params = parameter.numel()\n",
        "            total_params += params\n",
        "    return total_params\n",
        "\n",
        "def main():\n",
        "    # Initialize hyperparameters\n",
        "    vocab_size        = 20000\n",
        "    batch_size        = 64\n",
        "    d_model           = 32\n",
        "    ff_dim            = 32\n",
        "    max_len           = 32\n",
        "    num_epochs        = 10\n",
        "    num_runs          = 5\n",
        "    n_heads           = 4\n",
        "    n_layers          = 1\n",
        "    drop_p            = 0.1\n",
        "    val_split         = 0.1\n",
        "    layers            = ['SDPA','Optimised', 'Efficient', 'Super']\n",
        "\n",
        "    # Load data\n",
        "    train_dataloader, val_dataloader, test_dataloader = get_dataloader(\"./IMDB_Dataset.csv\",\n",
        "                                                                       vocab_size,\n",
        "                                                                       max_len,\n",
        "                                                                       batch_size,\n",
        "                                                                       val_split)\n",
        "    for layer_type in layers:\n",
        "        avg_train_loss  = 0\n",
        "        avg_train_acc   = 0\n",
        "        avg_test_loss   = 0\n",
        "        avg_test_acc    = 0\n",
        "        avg_model_size  = 0\n",
        "        num_params      = 0\n",
        "        run_times       = []\n",
        "        for _ in range(num_runs):\n",
        "            # Initialize model, optimizer, and criterion and train/test the model\n",
        "            print(f\"Working with layer type: {layer_type}\")\n",
        "            model_name  = 'best_model.pth'\n",
        "            transformer = Transformer(vocab_size=vocab_size,\n",
        "                                      n_heads=n_heads,\n",
        "                                      n_layers=n_layers,\n",
        "                                      d_model=d_model,\n",
        "                                      d_lin=ff_dim,\n",
        "                                      max_len=max_len,\n",
        "                                      drop_p = drop_p,\n",
        "                                      layer_type = layer_type)\n",
        "            transformer = transformer.to(device)\n",
        "            #Setup loss function and optimizer\n",
        "            loss_func = nn.BCEWithLogitsLoss()\n",
        "            #Performs slightly differently than Keras optimizer\n",
        "            optim = torch.optim.Adam(transformer.parameters(),lr=1e-3)\n",
        "            start_time       = time.time()\n",
        "            best_val_acc     = -float('inf')\n",
        "            for epoch in range(num_epochs):\n",
        "                train_loss, train_acc = train_loop(device,transformer,optim,train_dataloader,loss_func,epoch,True)\n",
        "                val_loss, val_acc     = train_loop(device,transformer,optim,val_dataloader,loss_func,epoch,False)\n",
        "                #Print Results per epoch\n",
        "                print(f\" Epoch {epoch}: Train loss: {round(train_loss.item(), 4)} |  Train acc: {round(train_acc.item(), 4)} | \\\n",
        "                Val loss: {round(val_loss.item(), 4)} | Val acc: {round(val_acc.item(), 4)}\")\n",
        "                #Check if our model improved\n",
        "                if val_acc >= best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    save_checkpoint(transformer, optim, epoch, best_val_acc, model_name)\n",
        "            end_time = time.time()\n",
        "            # Check the best models performance\n",
        "            best_model, best_optim, start_epoch, _ = load_checkpoint(transformer, optim, model_name)\n",
        "            test_loss, test_acc   = train_loop(device,best_model,best_optim,test_dataloader,loss_func,start_epoch,False)\n",
        "            print(f\"Best Model: Test Acc {test_acc} | Test Loss {test_loss} \\n\")\n",
        "            # Check the size of the best model\n",
        "            param_size  = 0\n",
        "            for param in best_model.parameters():\n",
        "                param_size  += param.nelement() * param.element_size()\n",
        "            buffer_size = 0\n",
        "            for buffer in best_model.buffers():\n",
        "                buffer_size += buffer.nelement() * buffer.element_size()\n",
        "            size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "            # Count number of parameters\n",
        "            num_params = count_parameters(transformer)\n",
        "            # Accumulate results\n",
        "            avg_train_loss += train_loss\n",
        "            avg_train_acc  += train_acc\n",
        "            avg_test_loss  += test_loss\n",
        "            avg_test_acc   += test_acc\n",
        "            avg_model_size += size_all_mb\n",
        "            run_times.append(end_time-start_time)\n",
        "        run_times.sort()\n",
        "        med_run_time = run_times[len(run_times) // 2]\n",
        "        file_name = f\"{layer_type}_results_final.txt\"\n",
        "        f = open(file_name,\"a\")\n",
        "        f.write(f\"Average Train Acc over {num_runs} for {layer_type}: {avg_train_acc / num_runs} \\n\")\n",
        "        f.write(f\"Average Train Loss over {num_runs} for {layer_type}: {avg_train_loss / num_runs} \\n\")\n",
        "        f.write(f\"Average Test Acc over {num_runs} for {layer_type}: {avg_test_acc / num_runs} \\n\")\n",
        "        f.write(f\"Average Test Loss over {num_runs} for {layer_type}: {avg_test_loss / num_runs} \\n\")\n",
        "        f.write(f\"Average Model Size over {num_runs} for {layer_type}: {avg_model_size / num_runs} \\n\")\n",
        "        f.write(f\"Median Run Time over {num_runs} for {layer_type}: {med_run_time} \\n\")\n",
        "        f.write(f\"Number of parameters: {num_params} \\n\")\n",
        "        f.write(\"\\n\")\n",
        "        f.close()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}