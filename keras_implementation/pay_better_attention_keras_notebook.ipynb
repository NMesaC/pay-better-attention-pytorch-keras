{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The default keras version on Google Colab does not support keras ops, so we upgrade it here. If you are running this locally and have the most recent keras version, you can skip this cell."
      ],
      "metadata": {
        "id": "Lx6Oo1xM-seB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10eGG_Zq-bD8",
        "outputId": "a4da2de8-9837-4e9a-88b6-7fb487102ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.4.1 namex-0.0.8 optree-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keras --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell has the Super Attention Layer that has a single W_a matrix managed by the Multi Head Attention Module. Run it if you want to use that version of Super Attention (all other attention layers are the same)"
      ],
      "metadata": {
        "id": "n0dL17sZ-rp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "\"\"\"\n",
        "## Attention Layers (Shared W_a)\n",
        "\"\"\"\n",
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 d_q: int,\n",
        "                 d_k: int,\n",
        "                 d_v: int,\n",
        "                 W_a: keras.layers.Dense = None,\n",
        "                 layer_type: str = 'SDPA',\n",
        "                 idx: int = 0,\n",
        "                 max_len: int = 32,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.d_q = d_q\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.layer_type = layer_type\n",
        "        self.idx = idx\n",
        "        self.max_len = max_len\n",
        "        self.W_a = W_a\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = layers.Dense(self.d_q)\n",
        "        if self.layer_type in ['SDPA', 'Optimised']:\n",
        "            self.W_k = layers.Dense(self.d_k)\n",
        "        if self.layer_type == 'SDPA':\n",
        "            self.W_v = layers.Dense(self.d_v)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs\n",
        "        if self.layer_type == 'Optimised':\n",
        "            return self._forward_optimised(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Efficient':\n",
        "            return self._forward_efficient(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Super':\n",
        "            return self._forward_super(inp_q, inp_k, inp_v)\n",
        "        else:\n",
        "            return self._forward_SDPA(inp_q, inp_k, inp_v)\n",
        "\n",
        "    def _forward_SDPA(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        V = self.W_v(inp_v)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_optimised(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        v_lo = self.idx * self.d_v\n",
        "        v_hi = (self.idx + 1) * self.d_v\n",
        "        V = inp_v[:, :, v_lo:v_hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_efficient(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_super(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = self.W_a.kernel @ inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, max_len, layer_type, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.max_len = max_len\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        if self.layer_type == 'Super':\n",
        "            self.W_a = layers.Dense(self.max_len)\n",
        "            self.W_a.build((None, self.max_len))\n",
        "        else:\n",
        "            self.W_a = None\n",
        "\n",
        "        self.attention_layers = [\n",
        "            AttentionLayer(self.d_model, self.d_k, self.d_k, self.d_v,\n",
        "                           self.W_a, self.layer_type, idx=i, max_len=self.max_len)\n",
        "            for i in range(self.n_heads)\n",
        "        ]\n",
        "\n",
        "        self.W_o = layers.Dense(self.d_model)\n",
        "\n",
        "        # Build each attention layer\n",
        "        for layer in self.attention_layers:\n",
        "            layer.build(input_shape)\n",
        "\n",
        "        # Build the output dense layer\n",
        "        self.W_o.build((None, self.n_heads * self.d_v))\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs, inputs, inputs\n",
        "\n",
        "        H = None\n",
        "        for i, layer in enumerate(self.attention_layers):\n",
        "            h_i = layer([inp_q, inp_k, inp_v])\n",
        "            if i == 0:\n",
        "                H = h_i\n",
        "            else:\n",
        "                H = tf.concat([H, h_i], axis=-1)\n",
        "\n",
        "        out = self.W_o(H)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f48ICSjT-pJz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell has the Super Attention Layer that has a single W_a matrix FOR EACH Super Attention Layer, independent of the Multi Head Attention Module. Run it if you want to use that version of Super Attention (all other attention layers are the same)"
      ],
      "metadata": {
        "id": "g1DmT4hj_LCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Attention Layers (Individual W_a)\n",
        "\"\"\"\n",
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 d_q: int,\n",
        "                 d_k: int,\n",
        "                 d_v: int,\n",
        "                 layer_type: str = 'SDPA',\n",
        "                 idx: int = 0,\n",
        "                 max_len: int = 32,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.d_q = d_q\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.layer_type = layer_type\n",
        "        self.idx = idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = layers.Dense(self.d_q)\n",
        "        self.W_q.build((None, self.d_model))\n",
        "        if self.layer_type in ['SDPA', 'Optimised']:\n",
        "            self.W_k = layers.Dense(self.d_k)\n",
        "            self.W_k.build((None, self.d_model))\n",
        "        if self.layer_type == 'SDPA':\n",
        "            self.W_v = layers.Dense(self.d_v)\n",
        "            self.W_v.build((None, self.d_model))\n",
        "        if self.layer_type == 'Super':\n",
        "            self.W_a = layers.Dense(self.max_len)\n",
        "            self.W_a.build((None, self.max_len))\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs\n",
        "        if self.layer_type == 'Optimised':\n",
        "            return self._forward_optimised(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Efficient':\n",
        "            return self._forward_efficient(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Super':\n",
        "            return self._forward_super(inp_q, inp_k, inp_v)\n",
        "        else:\n",
        "            return self._forward_SDPA(inp_q, inp_k, inp_v)\n",
        "\n",
        "    def _forward_SDPA(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        V = self.W_v(inp_v)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_optimised(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        v_lo = self.idx * self.d_v\n",
        "        v_hi = (self.idx + 1) * self.d_v\n",
        "        V = inp_v[:, :, v_lo:v_hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_efficient(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_super(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = self.W_a.kernel @ inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, max_len, layer_type, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.max_len = max_len\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.attention_layers = [\n",
        "            AttentionLayer(d_model=self.d_model,\n",
        "                           d_q=self.d_k,\n",
        "                           d_k=self.d_k,\n",
        "                           d_v=self.d_v,\n",
        "                           layer_type=self.layer_type,\n",
        "                           idx=i,\n",
        "                           max_len=self.max_len)\n",
        "            for i in range(self.n_heads)\n",
        "        ]\n",
        "\n",
        "        # Build each attention layer\n",
        "        for layer in self.attention_layers:\n",
        "            layer.build(input_shape)\n",
        "\n",
        "        # Build the output dense layer\n",
        "        self.W_o = layers.Dense(self.d_model)\n",
        "        self.W_o.build((None, self.n_heads * self.d_v))\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs, inputs, inputs\n",
        "\n",
        "        H = None\n",
        "        for i, layer in enumerate(self.attention_layers):\n",
        "            h_i = layer([inp_q, inp_k, inp_v])\n",
        "            if i == 0:\n",
        "                H = h_i\n",
        "            else:\n",
        "                H = tf.concat([H, h_i], axis=-1)\n",
        "\n",
        "        out = self.W_o(H)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pOSn9B1Y_MdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell runs the main script!"
      ],
      "metadata": {
        "id": "Gk8JNu4UBB6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## \"You Need to Pay Better Attention\" Keras Transformer Example\n",
        "\n",
        "## Paper Link: https://arxiv.org/abs/2403.01643\n",
        "\n",
        "## Author: Nicholas Mesa-Cucalon (https://github.com/NMesaC)\n",
        "\"\"\"\n",
        "import keras\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "keras.utils.set_random_seed(1019)\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, max_len, layer_type = 'SDPA', rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(n_heads=num_heads,\n",
        "                                      d_model=embed_dim,\n",
        "                                      d_k=embed_dim // num_heads,\n",
        "                                      d_v=embed_dim // num_heads,\n",
        "                                      max_len=max_len,\n",
        "                                      layer_type=layer_type)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "def get_model_size(model):\n",
        "    # Save the model to a temporary file\n",
        "    _, keras_file = tempfile.mkstemp('.h5')\n",
        "    model.save(keras_file, include_optimizer=True)\n",
        "\n",
        "    # Get the file size\n",
        "    size_bytes = os.path.getsize(keras_file)\n",
        "\n",
        "    # Convert to MB\n",
        "    size_mb = size_bytes / (1024 * 1024)\n",
        "\n",
        "    # Delete the temporary file\n",
        "    os.remove(keras_file)\n",
        "\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Setup initial variables\n",
        "    vocab_size = 20000  # Only consider the top 20k words\n",
        "    maxlen     = 32     # Only consider the first 32 words of each movie review\n",
        "    embed_dim  = 32     # Embedding size for each token\n",
        "    ff_dim     = 32     # Hidden layer size in feed forward network inside transformer\n",
        "    batch_size = 64     # Batch Size\n",
        "    epochs     = 10     # Number of epochs\n",
        "    num_heads  = 4      # Number of attention heads\n",
        "    # The IMDB dataset is slightly different between Keras and PyTorch, so the results are slightly different\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "    print(len(x_train), \"Training sequences\")\n",
        "    x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "    x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "    # Train a Transformer Model with Each Attention Layer Type\n",
        "    num_runs = 5\n",
        "    layer_types = ['SDPA','Optimised','Efficient','Super']\n",
        "    for layer in layer_types:\n",
        "        avg_test_acc, avg_test_loss, avg_model_size = 0, 0, 0\n",
        "        run_times = []\n",
        "        for run in range(num_runs):\n",
        "            print(f\"Training with {layer} training layer\")\n",
        "            # Create inputs\n",
        "            inputs = layers.Input(shape=(maxlen,))\n",
        "            # Create model\n",
        "            embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "            x = embedding_layer(inputs)\n",
        "            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, maxlen, layer)\n",
        "            x = transformer_block(x)\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "            x = layers.Dropout(0.1)(x)\n",
        "            x = layers.Dense(6, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(0.1)(x)\n",
        "            outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "            # Initialize model\n",
        "            model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "            model.compile(\n",
        "                optimizer=\"adam\", loss=\"BCE\", metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "            # Create callbacks to save best model\n",
        "            checkpoint_filepath = \"./results/imdb/model/\" + layer + f\"_{num_heads}_heads\" + \"/run_num_\" + str(run) + \"/\" + \"model.weights.h5\"\n",
        "            os.makedirs(os.path.dirname(checkpoint_filepath), exist_ok=True)\n",
        "            # NOTE: Keras callback will save only the weights of the transformer model when val_accuracy goes up\n",
        "            checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_filepath,\n",
        "                                                                  monitor=\"val_accuracy\",\n",
        "                                                                  save_best_only=True,\n",
        "                                                                  save_weights_only=True,\n",
        "                                                                 )\n",
        "            # Fit model and retrieve history + time taken\n",
        "            start_time = time.time()\n",
        "            history = model.fit(x=x_train,\n",
        "                                y=y_train,\n",
        "                                batch_size=batch_size,\n",
        "                                epochs=epochs,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[checkpoint_callback])\n",
        "            end_time = time.time()\n",
        "            duration = end_time - start_time\n",
        "            run_times.append(duration)\n",
        "            print(f\"Training took {duration:.4f} seconds\")\n",
        "\n",
        "            # Calculate model size\n",
        "            model_size = get_model_size(model)\n",
        "            print(f\"The size of the model is approximately {model_size:.4f} MB\")\n",
        "\n",
        "            # Compute number of attention parameters\n",
        "            num_of_attention_params = model.layers[2].att.count_params()\n",
        "            print(f\"Number of attention params: {num_of_attention_params}\")\n",
        "\n",
        "            # Compute and display test loss and accuracy for best model\n",
        "            model.load_weights(checkpoint_filepath)\n",
        "            test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "            print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\\n\")\n",
        "            avg_test_loss  += test_loss\n",
        "            avg_test_acc   += test_acc\n",
        "            avg_model_size += model_size\n",
        "        run_times.sort()\n",
        "        med_run_time = run_times[len(run_times) // 2]\n",
        "        file_name = f\"{layer}_results.txt\"\n",
        "        f = open(file_name,\"a\")\n",
        "        f.write(f\"Average Test Acc over {num_runs} for {layer}: {avg_test_acc / num_runs} \\n\")\n",
        "        f.write(f\"Average Test Loss over {num_runs} for {layer}: {avg_test_loss / num_runs} \\n\")\n",
        "        f.write(f\"Average Model Size over {num_runs} for {layer}: {avg_model_size / num_runs} \\n\")\n",
        "        f.write(f\"Median Run Time over {num_runs} for {layer}: {med_run_time} \\n\")\n",
        "        f.write(f\"Number of attention parameters for {layer}: {num_of_attention_params} \\n\")\n",
        "        f.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxuNRihK-gNJ",
        "outputId": "5a38c3b7-d022-448c-b9a7-6f2ebbbe05fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "25000 Training sequences\n",
            "Training with SDPA training layer\n",
            "Epoch 1/10\n"
          ]
        }
      ]
    }
  ]
}