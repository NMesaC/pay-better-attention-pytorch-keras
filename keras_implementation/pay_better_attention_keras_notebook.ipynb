{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The default keras version on Google Colab does not support keras ops, so we upgrade it here. If you are running this locally and have the most recent keras version, you can skip this cell."
      ],
      "metadata": {
        "id": "Lx6Oo1xM-seB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10eGG_Zq-bD8",
        "outputId": "dba5d68a-df84-4c36-b6cc-53b841b708aa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.4.1 namex-0.0.8 optree-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keras --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell has the Super Attention Layer that has a single W_a matrix managed by the Multi Head Attention Module. Run it if you want to use that version of Super Attention (all other attention layers are the same)"
      ],
      "metadata": {
        "id": "n0dL17sZ-rp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "\"\"\"\n",
        "## Attention Layers (Shared W_a)\n",
        "\"\"\"\n",
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 d_q: int,\n",
        "                 d_k: int,\n",
        "                 d_v: int,\n",
        "                 W_a: keras.layers.Dense = None,\n",
        "                 layer_type: str = 'SDPA',\n",
        "                 idx: int = 0,\n",
        "                 max_len: int = 32,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.d_q = d_q\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.layer_type = layer_type\n",
        "        self.idx = idx\n",
        "        self.max_len = max_len\n",
        "        self.W_a = W_a\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = layers.Dense(self.d_q)\n",
        "        if self.layer_type in ['SDPA', 'Optimised']:\n",
        "            self.W_k = layers.Dense(self.d_k)\n",
        "        if self.layer_type == 'SDPA':\n",
        "            self.W_v = layers.Dense(self.d_v)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs\n",
        "        if self.layer_type == 'Optimised':\n",
        "            return self._forward_optimised(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Efficient':\n",
        "            return self._forward_efficient(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Super':\n",
        "            return self._forward_super(inp_q, inp_k, inp_v)\n",
        "        else:\n",
        "            return self._forward_SDPA(inp_q, inp_k, inp_v)\n",
        "\n",
        "    def _forward_SDPA(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        V = self.W_v(inp_v)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_optimised(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        v_lo = self.idx * self.d_v\n",
        "        v_hi = (self.idx + 1) * self.d_v\n",
        "        V = inp_v[:, :, v_lo:v_hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_efficient(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_super(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = self.W_a.kernel @ inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, max_len, layer_type, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.max_len = max_len\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        if self.layer_type == 'Super':\n",
        "            self.W_a = layers.Dense(self.max_len)\n",
        "            self.W_a.build((None, self.max_len))\n",
        "        else:\n",
        "            self.W_a = None\n",
        "\n",
        "        self.attention_layers = [\n",
        "            AttentionLayer(self.d_model, self.d_k, self.d_k, self.d_v,\n",
        "                           self.W_a, self.layer_type, idx=i, max_len=self.max_len)\n",
        "            for i in range(self.n_heads)\n",
        "        ]\n",
        "\n",
        "        self.W_o = layers.Dense(self.d_model)\n",
        "\n",
        "        # Build each attention layer\n",
        "        for layer in self.attention_layers:\n",
        "            layer.build(input_shape)\n",
        "\n",
        "        # Build the output dense layer\n",
        "        self.W_o.build((None, self.n_heads * self.d_v))\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs, inputs, inputs\n",
        "\n",
        "        H = None\n",
        "        for i, layer in enumerate(self.attention_layers):\n",
        "            h_i = layer([inp_q, inp_k, inp_v])\n",
        "            if i == 0:\n",
        "                H = h_i\n",
        "            else:\n",
        "                H = tf.concat([H, h_i], axis=-1)\n",
        "\n",
        "        out = self.W_o(H)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f48ICSjT-pJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell has the Super Attention Layer that has a single W_a matrix FOR EACH Super Attention Layer, independent of the Multi Head Attention Module. Run it if you want to use that version of Super Attention (all other attention layers are the same)"
      ],
      "metadata": {
        "id": "g1DmT4hj_LCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Attention Layers (Individual W_a)\n",
        "\"\"\"\n",
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 d_q: int,\n",
        "                 d_k: int,\n",
        "                 d_v: int,\n",
        "                 layer_type: str = 'SDPA',\n",
        "                 idx: int = 0,\n",
        "                 max_len: int = 32,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.d_q = d_q\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.layer_type = layer_type\n",
        "        self.idx = idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = layers.Dense(self.d_q)\n",
        "        self.W_q.build((None, self.d_model))\n",
        "        if self.layer_type in ['SDPA', 'Optimised']:\n",
        "            self.W_k = layers.Dense(self.d_k)\n",
        "            self.W_k.build((None, self.d_model))\n",
        "        if self.layer_type == 'SDPA':\n",
        "            self.W_v = layers.Dense(self.d_v)\n",
        "            self.W_v.build((None, self.d_model))\n",
        "        if self.layer_type == 'Super':\n",
        "            self.W_a = layers.Dense(self.max_len)\n",
        "            self.W_a.build((None, self.max_len))\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs\n",
        "        if self.layer_type == 'Optimised':\n",
        "            return self._forward_optimised(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Efficient':\n",
        "            return self._forward_efficient(inp_q, inp_k, inp_v)\n",
        "        elif self.layer_type == 'Super':\n",
        "            return self._forward_super(inp_q, inp_k, inp_v)\n",
        "        else:\n",
        "            return self._forward_SDPA(inp_q, inp_k, inp_v)\n",
        "\n",
        "    def _forward_SDPA(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        V = self.W_v(inp_v)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_optimised(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        K = self.W_k(inp_k)\n",
        "        K_t = tf.transpose(K, perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        v_lo = self.idx * self.d_v\n",
        "        v_hi = (self.idx + 1) * self.d_v\n",
        "        V = inp_v[:, :, v_lo:v_hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_efficient(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "    def _forward_super(self, inp_q, inp_k, inp_v):\n",
        "        Q = self.W_q(inp_q)\n",
        "        lo = self.idx * self.d_k\n",
        "        hi = (self.idx + 1) * self.d_k\n",
        "        K_t = tf.transpose(inp_k[:, :, lo:hi], perm=[0, 2, 1])\n",
        "        S = tf.nn.softmax((Q @ K_t) / tf.math.sqrt(tf.cast(self.d_q, tf.float32)), axis=1)\n",
        "        V = self.W_a.kernel @ inp_v[:, :, lo:hi]\n",
        "        H = S @ V\n",
        "        return H\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, n_heads, d_model, d_k, d_v, max_len, layer_type, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.max_len = max_len\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.attention_layers = [\n",
        "            AttentionLayer(d_model=self.d_model,\n",
        "                           d_q=self.d_k,\n",
        "                           d_k=self.d_k,\n",
        "                           d_v=self.d_v,\n",
        "                           layer_type=self.layer_type,\n",
        "                           idx=i,\n",
        "                           max_len=self.max_len)\n",
        "            for i in range(self.n_heads)\n",
        "        ]\n",
        "\n",
        "        # Build each attention layer\n",
        "        for layer in self.attention_layers:\n",
        "            layer.build(input_shape)\n",
        "\n",
        "        # Build the output dense layer\n",
        "        self.W_o = layers.Dense(self.d_model)\n",
        "        self.W_o.build((None, self.n_heads * self.d_v))\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inp_q, inp_k, inp_v = inputs, inputs, inputs\n",
        "\n",
        "        H = None\n",
        "        for i, layer in enumerate(self.attention_layers):\n",
        "            h_i = layer([inp_q, inp_k, inp_v])\n",
        "            if i == 0:\n",
        "                H = h_i\n",
        "            else:\n",
        "                H = tf.concat([H, h_i], axis=-1)\n",
        "\n",
        "        out = self.W_o(H)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pOSn9B1Y_MdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell runs the main script!"
      ],
      "metadata": {
        "id": "Gk8JNu4UBB6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## \"You Need to Pay Better Attention\" Keras Transformer Example\n",
        "\n",
        "## Paper Link: https://arxiv.org/abs/2403.01643\n",
        "\n",
        "## Author: Nicholas Mesa-Cucalon (https://github.com/NMesaC)\n",
        "\"\"\"\n",
        "import keras\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "keras.utils.set_random_seed(1019)\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, max_len, layer_type = 'SDPA', rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(n_heads=num_heads,\n",
        "                                      d_model=embed_dim,\n",
        "                                      d_k=embed_dim // num_heads,\n",
        "                                      d_v=embed_dim // num_heads,\n",
        "                                      max_len=max_len,\n",
        "                                      layer_type=layer_type)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "def get_model_size(model):\n",
        "    # Save the model to a temporary file\n",
        "    _, keras_file = tempfile.mkstemp('.h5')\n",
        "    model.save(keras_file, include_optimizer=True)\n",
        "\n",
        "    # Get the file size\n",
        "    size_bytes = os.path.getsize(keras_file)\n",
        "\n",
        "    # Convert to MB\n",
        "    size_mb = size_bytes / (1024 * 1024)\n",
        "\n",
        "    # Delete the temporary file\n",
        "    os.remove(keras_file)\n",
        "\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Setup initial variables\n",
        "    vocab_size = 20000  # Only consider the top 20k words\n",
        "    maxlen     = 32     # Only consider the first 32 words of each movie review\n",
        "    embed_dim  = 32     # Embedding size for each token\n",
        "    ff_dim     = 32     # Hidden layer size in feed forward network inside transformer\n",
        "    batch_size = 64     # Batch Size\n",
        "    epochs     = 10     # Number of epochs\n",
        "    num_heads  = 4      # Number of attention heads\n",
        "    # The IMDB dataset is slightly different between Keras and PyTorch, so the results are slightly different\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "    print(len(x_train), \"Training sequences\")\n",
        "    x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "    x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "    # Train a Transformer Model with Each Attention Layer Type\n",
        "    num_runs = 5\n",
        "    layer_types = ['SDPA','Optimised','Efficient','Super']\n",
        "    for layer in layer_types:\n",
        "        avg_test_acc, avg_test_loss   = 0, 0\n",
        "        avg_run_time, avg_model_size  = 0, 0\n",
        "        for run in range(num_runs):\n",
        "            print(f\"Training with {layer} training layer\")\n",
        "            # Create inputs\n",
        "            inputs = layers.Input(shape=(maxlen,))\n",
        "            # Create model\n",
        "            embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "            x = embedding_layer(inputs)\n",
        "            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, maxlen, layer)\n",
        "            x = transformer_block(x)\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "            x = layers.Dropout(0.1)(x)\n",
        "            x = layers.Dense(6, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(0.1)(x)\n",
        "            outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "            # Initialize model\n",
        "            model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "            model.compile(\n",
        "                optimizer=\"adam\", loss=\"BCE\", metrics=[\"accuracy\"]\n",
        "            )\n",
        "\n",
        "            # Create callbacks to save best model\n",
        "            checkpoint_filepath = \"./results/imdb/model/\" + layer + f\"_{num_heads}_heads\" + \"/run_num_\" + str(run) + \"/\" + \"model.weights.h5\"\n",
        "            os.makedirs(os.path.dirname(checkpoint_filepath), exist_ok=True)\n",
        "            # NOTE: Keras callback will save only the weights of the transformer model when val_accuracy goes up\n",
        "            checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_filepath,\n",
        "                                                                  monitor=\"val_accuracy\",\n",
        "                                                                  save_best_only=True,\n",
        "                                                                  save_weights_only=True,\n",
        "                                                                 )\n",
        "            # Fit model and retrieve history + time taken\n",
        "            start_time = time.time()\n",
        "            history = model.fit(x=x_train,\n",
        "                                y=y_train,\n",
        "                                batch_size=batch_size,\n",
        "                                epochs=epochs,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[checkpoint_callback])\n",
        "            end_time = time.time()\n",
        "            duration = end_time - start_time\n",
        "            avg_run_time += duration\n",
        "            print(f\"Training took {duration:.4f} seconds\")\n",
        "\n",
        "            # Calculate model size\n",
        "            model_size = get_model_size(model)\n",
        "            print(f\"The size of the model is approximately {model_size:.4f} MB\")\n",
        "\n",
        "            # Compute number of attention parameters\n",
        "            num_of_attention_params = model.layers[2].att.count_params()\n",
        "            print(f\"Number of attention params: {num_of_attention_params}\")\n",
        "\n",
        "            # Compute and display test loss and accuracy for best model\n",
        "            model.load_weights(checkpoint_filepath)\n",
        "            test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "            print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\\n\")\n",
        "            avg_test_loss  += test_loss\n",
        "            avg_test_acc   += test_acc\n",
        "            avg_model_size += model_size\n",
        "        file_name = f\"{layer}_results.txt\"\n",
        "        f = open(file_name,\"a\")\n",
        "        f.write(f\"Average Test Acc over {num_runs} for {layer}: {avg_test_acc / num_runs} \\n\")\n",
        "        f.write(f\"Average Test Loss over {num_runs} for {layer}: {avg_test_loss / num_runs} \\n\")\n",
        "        f.write(f\"Average Run Time over {num_runs} for {layer}: {avg_run_time / num_runs} \\n\")\n",
        "        f.write(f\"Average Model Size over {num_runs} for {layer}: {avg_model_size / num_runs} \\n\")\n",
        "        f.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "CxuNRihK-gNJ",
        "outputId": "c5013561-7e65-4490-b3f4-90c3f140e04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "Training with SDPA training layer\n",
            "Epoch 1/10\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - accuracy: 0.6597 - loss: 0.5935 - val_accuracy: 0.7768 - val_loss: 0.4541\n",
            "Epoch 2/10\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.8282 - loss: 0.3997 - val_accuracy: 0.7692 - val_loss: 0.5001\n",
            "Epoch 3/10\n",
            "\u001b[1m 79/352\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.8617 - loss: 0.3286"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-da0a39588478>\u001b[0m in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-da0a39588478>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Fit model and retrieve history + time taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             history = model.fit(x=x_train, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}